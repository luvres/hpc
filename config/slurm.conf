#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=hpcc
#ControlMachine=warewulf
ControlMachine=ip-172-3-255-27
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#
# GRES
GresTypes=gpu
#
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_ONE_TASK_PER_CORE
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log
#JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# OpenHPC default configuration
TaskPlugin=task/affinity
PropagateResourceLimitsExcept=MEMLOCK
JobCompType=jobcomp/filetxt
Epilog=/etc/slurm/slurm.epilog.clean
#
# NONE CONFIGURATIONS
#NodeName=cn51.exacta.eu.org Sockets=1 CoresPerSocket=8 ThreadsPerCore=1 CPUs=8 RealMemory=12288 Gres=gpu:gtx3090:1 Feature=GPU State=UNKNOWN
# -----------
#NodeName=cn52.exacta.eu.org Sockets=1 CoresPerSocket=8 ThreadsPerCore=1 CPUs=8 RealMemory=12288 Gres=gpu:titanv:1 Feature=GPU State=UNKNOWN
# -----------
NodeName=cn53.exacta.eu.org Sockets=1 CoresPerSocket=8 ThreadsPerCore=1 CPUs=8 RealMemory=64396 Gres=gpu:titanrtx:1 Feature=GPU State=UNKNOWN
# -----------
NodeName=cn54.exacta.eu.org Sockets=1 CoresPerSocket=8 ThreadsPerCore=1 CPUs=8 RealMemory=16018 Feature=Apps State=UNKNOWN
# ----------------------
#NodeName=cn81 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 CPUs=4 RealMemory=12288 Gres=gpu:gtx1060:1 Feature=Gpu State=UNKNOWN
#NodeName=cn82 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 CPUs=4 RealMemory=12288 Gres=gpu:gtx1050:1 Feature=Gpu State=UNKNOWN
# -----------
#NodeName=cn81 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 CPUs=4 RealMemory=12288 NodeHostName=cn81 NodeAddr=40.6.18.81 Gres=gpu:gtx1050:1 Feature=Gpu State=UNKNOWN
# ----------------------
#
# PARTITION CONFIGURATIONS
# -----------
PartitionName=gpu-titan-rtx Nodes=cn53.exacta.eu.org Default=NO MaxTime=0-24:00:00 State=UP Oversubscribe=EXCLUSIVE
# -----------
PartitionName=apps Nodes=cn54.exacta.eu.org Default=NO MaxTime=0-24:00:00 State=UP Oversubscribe=FORCE
# ----------------------
#PartitionName=normal Nodes=cn81,cn82 Default=YES MaxTime=0-24:00:00 State=UP Oversubscribe=EXCLUSIVE
#PartitionName=cn-gpu Nodes=cn81 Default=NO MaxTime=INFINITE State=UP
# ----------------------
#
SlurmctldParameters=enable_configless
#
ReturnToService=2
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300
